{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento e Validação da IA\n",
        "\n",
        "Download das bibliotecas, configuração do ambiente e import das bibliotecas e ferramentas que serão usadas\n",
        "Obs: Acesse Runtime e selecione a opção da GPU, para utilizar CUDA durante o treinamento da IA"
      ],
      "metadata": {
        "id": "VNzjJI8ryT4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRT34DR1yPKs"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y pyarrow requests fsspec\n",
        "!pip install pyarrow==14.0.1 requests==2.31.0 fsspec==2024.6.1\n",
        "!pip install cudf-cu12 gcsfs google-colab ibis-framework"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch seqeval pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pRAL8TD83OZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from sklearn.metrics import classification_report\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import AdamW, get_scheduler\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import DataCollatorForTokenClassification"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JpRoUgleyj1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "e8Zb-_HX0Lrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregar Dataset\n",
        "\n",
        "Carregar tadaset (presente nos arquivos **news_data.parquet** e **annot.parquet**. Estes arquivos são obtidos após a conversão dos dados dos arquivos json para parquet)"
      ],
      "metadata": {
        "id": "n-6nIFr0yjPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = Dataset.from_parquet('news_data.parquet')\n",
        "annotated_dataset = Dataset.from_parquet('annot.parquet')\n",
        "\n",
        "print(\"Head do dataset puro:\")\n",
        "print(news_dataset[:5])\n",
        "print(\"Head do dataset anotado:\")\n",
        "print(annotated_dataset[:5])\n",
        "\n",
        "print(\"Estrutura do dataset:\")\n",
        "print(news_dataset)\n",
        "print(\"Estrutura do dataset anotado:\")\n",
        "print(annotated_dataset)"
      ],
      "metadata": {
        "id": "wubd48owyg_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamento e preparação dos dados para o treinamento\n",
        "\n",
        "Primeiro, garantimos a integridade dos rótulos (ner_tags) para que estejam configurados em string. Garantimos anteriormente a integridade para o uso das ferramentas da bilblioteca dataset da HuggingFace, pois ela trabalha com arquivos no formato da Apache Arrow, e por isso estamos utilizando dados em parquet.\n",
        "Mapeamos os rótulos de string para IDs numéricos e montamos um Tokenizer e colocamos o acesso ao modelo BERT large, disponibilizado na HuggingFace. Usamos o BERTimbau por ele foi treinado para ser utilizado em problemas de Token Classification em português.\n",
        "Após isso, alinhamos as labels e tokenizamos o dataset do BERT."
      ],
      "metadata": {
        "id": "k-d_tp28zT-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_str_labels(dataset):\n",
        "    def to_str_labels(batch):\n",
        "        batch['ner_tags'] = [[str(label) for label in labels] for labels in batch['ner_tags']]\n",
        "        return batch\n",
        "    return dataset.map(to_str_labels, batched=True)\n",
        "\n",
        "annotated_dataset = ensure_str_labels(annotated_dataset)\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(set(tag for tags in annotated_dataset['ner_tags'] for tag in tags))}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "bert_model_name = 'neuralmind/bert-large-portuguese-cased'\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForTokenClassification.from_pretrained(bert_model_name, num_labels=len(label2id))\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = bert_tokenizer(\n",
        "        examples['tokens'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  #Ignorar tokens especiais\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label[word_idx]])  #Converter rótulo de string para ID\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets_bert = annotated_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"Primeiras 5 linhas do dataset tokenizado:\")\n",
        "print(tokenized_datasets_bert[:5])\n"
      ],
      "metadata": {
        "id": "3ba8TLRPzUH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento e Avaliação do modelo\n",
        "\n",
        "Aqui realizamos o treinamento do nosso modelo e efetuamos a avaliação, para verificar se tudo correu da forma esperada. Primeiro, preparamos os dados para o DataLoader. Configuramos o tamanho do batch (coloquei em 4 porque antes ocorreram problemas com a GPU). Após isso, montamos o DataLoader para treinamento e avaliação, efetuamos as configurações, como o número de épocas (deixei em 10 para ele conseguir ter contato com os outros rótulos que não fossem NULL (O) ), e realizamos o treinamento. Após o treinamento, avaliamos a precisão do modelo, usamos f1-score para ver como foi a performance, assim como a sua precisão, e salvamos o modelo e o tokenizer em uma pasta separada, pois os usaremos para rotular o restante do dataset."
      ],
      "metadata": {
        "id": "0AtnYo7w0PRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in batch])\n",
        "    attention_mask = torch.stack([torch.tensor(x['attention_mask']) for x in batch])\n",
        "    labels = torch.stack([torch.tensor(x['labels']) for x in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_datasets_bert, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "eval_dataloader = DataLoader(tokenized_datasets_bert, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
        "num_epochs = 10\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "def train(model, dataloader, optimizer, lr_scheduler):\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        del batch, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "        del batch, outputs, predictions, labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "bert_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train(bert_model, train_dataloader, optimizer, lr_scheduler)\n",
        "\n",
        "predictions, labels = evaluate(bert_model, eval_dataloader)\n",
        "\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
        "\n",
        "true_predictions = [\n",
        "    [id2label[pred] for pred, label in zip(prediction, label) if label != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [id2label[label] for pred, label in zip(prediction, label) if label != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
        "\n",
        "results = classification_report(flat_true_labels, flat_true_predictions, output_dict=True)\n",
        "print(\"Evaluation Results:\", results)\n",
        "\n",
        "model_save_path = './saved_model/model'\n",
        "bert_model.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Modelo salvo\")\n"
      ],
      "metadata": {
        "id": "2V7qfTtu0PXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
        "tokenizer_save_path = './saved_model/tokenizer'\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "id": "_qN9CSSu0Qz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotulação dos dados restantes\n",
        "\n",
        "Agora que temos um modelo já salvo, utilizamos o modelo e o Tokenizer para rotular o restante dos dados do dataset, que não estavam rotulados. Como o modelo foi refinado anteriormente, agora utilizamos o modelo para rotular os dados e salvar em um arquivo parquet, para tentar preservar sua integridade."
      ],
      "metadata": {
        "id": "CAYUdu4a0g7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_save_path = './saved_model/model'\n",
        "tokenizer_save_path = './saved_model/tokenizer'\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)\n",
        "model = BertForTokenClassification.from_pretrained(model_save_path).to(device)\n",
        "\n",
        "id2label = model.config.id2label\n",
        "\n",
        "def label_text(text):\n",
        "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).squeeze().cpu().tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze().cpu().tolist())\n",
        "    labels = [id2label[pred] for pred in predictions]\n",
        "    return list(zip(tokens, labels))\n",
        "\n",
        "news_data = pd.read_parquet('news_data.parquet')\n",
        "\n",
        "results = []\n",
        "for index, row in news_data.iterrows():\n",
        "    labeled_text = label_text(row['content'])\n",
        "    results.append({'title': row['title'], 'url': row['url'], 'date': row['date'], 'content': labeled_text})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "df_results.to_parquet('labeled_news_data.parquet', index=False)\n",
        "\n",
        "print(\"Dataset rotulado e salvo com sucesso!\")"
      ],
      "metadata": {
        "id": "_8Q-SGAT0hAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversão para outras formas\n",
        "\n",
        "Por fim, converti os arquivos para ler em outras formas os dados e poder prosseguir com as próximas etapas. O Label-Studio não lê arquivos em parquet, por exemplo, então eu precisava deixar em outros formatos para poder fazer o Pos Annotation por lá."
      ],
      "metadata": {
        "id": "GAkHIT6I0iZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_file = 'labeled_news_data.parquet'\n",
        "df = pd.read_parquet(parquet_file)\n",
        "\n",
        "df.to_json('labeled_news_data.json', orient='records', lines=True, force_ascii=False, indent=4)\n",
        "print(f\"JSON salvo)\n",
        "\n",
        "df.to_csv('labeled_news_data.csv', index=False)\n",
        "print(f\"CSV salvo\")\n",
        "\n",
        "df.to_csv('labeled_news_data.txt', index=False, sep='\\t')\n",
        "print(\"TXT salvo\")\n"
      ],
      "metadata": {
        "id": "TsMToxdJ0ifI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}